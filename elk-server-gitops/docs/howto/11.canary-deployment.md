# Canary Deployment

Este guia mostra como usar o Istio e o Flagger para automatizar Canary deploymnet.

Pré-requisitos
O Flagger requer um cluster Kubernetes v1.16 ou mais recente e o Istio v1.5 ou mais recente.

Instale o Flagger no namespace:istio-system

base > kustomization.yaml
```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
#- app
- monitoring
- service_mesh
#- velero
#- db
- flagger

```
Crie um gateway de entrada para expor o aplicativo de demonstração fora da malha:
```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: public-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - "*"
```

O Flagger usa uma implantação do Kubernetes e, opcionalmente, um HPA (pod autoscaler horizontal) e, em seguida, cria uma série de objetos (implantações do Kubernetes, serviços ClusterIP, regras de destino do Istio e serviços virtuais). Esses objetos expõem a aplicação dentro da malha e conduzem a análise e a promoção do canário.

Crie um namespace de teste com a injeção de sidecar do Istio habilitada:
```
kubectl create ns test
kubectl label namespace test istio-injection=enabled
```
Crie uma implantação e um autoscaler de pod horizontal:
```
kubectl apply -k https://github.com/fluxcd/flagger//kustomize/podinfo?ref=main
```
Implante o serviço de teste de carga para gerar tráfego durante a análise canária:

```
kubectl apply -k https://github.com/fluxcd/flagger//kustomize/tester?ref=main
```
Crie um recurso personalizado canário (substitua example.com por seu próprio domínio):
```
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: podinfo
  namespace: test
spec:
  # deployment reference
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  # the maximum time in seconds for the canary deployment
  # to make progress before it is rollback (default 600s)
  progressDeadlineSeconds: 60
  # HPA reference (optional)
  autoscalerRef:
    apiVersion: autoscaling/v2beta2
    kind: HorizontalPodAutoscaler
    name: podinfo
  service:
    # service port number
    port: 9898
    # container port number or name (optional)
    targetPort: 9898
    # Istio gateways (optional)
    gateways:
    - public-gateway.istio-system.svc.cluster.local
    # Istio virtual service host names (optional)
    hosts:
    - app.example.com
    # Istio traffic policy (optional)
    trafficPolicy:
      tls:
        # use ISTIO_MUTUAL when mTLS is enabled
        mode: DISABLE
    # Istio retry policy (optional)
    retries:
      attempts: 3
      perTryTimeout: 1s
      retryOn: "gateway-error,connect-failure,refused-stream"
  analysis:
    # schedule interval (default 60s)
    interval: 1m
    # max number of failed metric checks before rollback
    threshold: 5
    # max traffic percentage routed to canary
    # percentage (0-100)
    maxWeight: 50
    # canary increment step
    # percentage (0-100)
    stepWeight: 10
    metrics:
    - name: request-success-rate
      # minimum req success rate (non 5xx responses)
      # percentage (0-100)
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      # maximum req duration P99
      # milliseconds
      thresholdRange:
        max: 500
      interval: 30s
    # testing (optional)
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 30s
        metadata:
          type: bash
          cmd: "curl -sd 'test' http://podinfo-canary:9898/token | grep token"
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://podinfo-canary.test:9898/"
```
Observe que ao usar o Istio 1.4 você precisa substituir o por um modelo de métrica.request-duration

Salve o recurso acima como podinfo-canary.yaml e aplique-o:

kubectl apply -f ./podinfo-canary.yaml

Quando a análise do canário for iniciada, o Flagger chamará os webhooks de pré-implantação antes de rotear o tráfego para o canário. A análise canary será executada por cinco minutos enquanto valida as métricas HTTP e os ganchos de distribuição a cada minuto.

Objetos Flagger

Após alguns segundos, o Flagger criará os objetos canários:

# applied 
deployment.apps/podinfo
horizontalpodautoscaler.autoscaling/podinfo
canary.flagger.app/podinfo

# generated 
deployment.apps/podinfo-primary
horizontalpodautoscaler.autoscaling/podinfo-primary
service/podinfo
service/podinfo-canary
service/podinfo-primary
destinationrule.networking.istio.io/podinfo-canary
destinationrule.networking.istio.io/podinfo-primary
virtualservice.networking.istio.io/podinfo

Promoção de canário automatizado

Acione uma implantação canária atualizando a imagem do contêiner:
```
kubectl -n test set image deployment/podinfo \
podinfod=ghcr.io/stefanprodan/podinfo:6.0.1
```

O Flagger detecta que a revisão de implantação foi alterada e inicia uma nova distribuição:
```
kubectl -n test describe canary/podinfo
```
```
Status:
  Canary Weight:         0
  Failed Checks:         0
  Phase:                 Succeeded
Events:
  Type     Reason  Age   From     Message
  ----     ------  ----  ----     -------
  Normal   Synced  3m    flagger  New revision detected podinfo.test
  Normal   Synced  3m    flagger  Scaling up podinfo.test
  Warning  Synced  3m    flagger  Waiting for podinfo.test rollout to finish: 0 of 1 updated replicas are available
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 5
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 10
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 15
  Normal   Synced  2m    flagger  Advance podinfo.test canary weight 20
  Normal   Synced  2m    flagger  Advance podinfo.test canary weight 25
  Normal   Synced  1m    flagger  Advance podinfo.test canary weight 30
  Normal   Synced  1m    flagger  Advance podinfo.test canary weight 35
  Normal   Synced  55s   flagger  Advance podinfo.test canary weight 40
  Normal   Synced  45s   flagger  Advance podinfo.test canary weight 45
  Normal   Synced  35s   flagger  Advance podinfo.test canary weight 50
  Normal   Synced  25s   flagger  Copying podinfo.test template spec to podinfo-primary.test
  Warning  Synced  15s   flagger  Waiting for podinfo-primary.test rollout to finish: 1 of 2 updated replicas are available
  Normal   Synced  5s    flagger  Promotion completed! Scaling down podinfo.test
```
Observe que, se você aplicar novas alterações à implantação durante a análise canária, o Flagger reiniciará a análise.

Uma implantação canária é acionada por alterações em qualquer um dos seguintes objetos:

Implantação PodSpec (imagem de contêiner, comando, portas, env, recursos, etc)
ConfigMaps montados como volumes ou mapeados para variáveis de ambiente
Segredos montados como volumes ou mapeados para variáveis de ambiente
Você pode monitorar todos os canários com:

```
watch kubectl get canaries --all-namespaces

NAMESPACE   NAME      STATUS        WEIGHT   LASTTRANSITIONTIME
test        podinfo   Progressing   15       2019-01-16T14:05:07Z
prod        frontend  Succeeded     0        2019-01-15T16:15:07Z
prod        backend   Failed        0        2019-01-14T17:05:07Z
```
Reversão automatizada
Durante a análise canária, você pode gerar erros HTTP 500 e alta latência para testar se o Flagger pausa a distribuição.

Acione outra implantação canária:
```
kubectl -n test set image deployment/podinfo \
podinfod=ghcr.io/stefanprodan/podinfo:6.0.2
```
Exec no pod do testador de carga com:
```
kubectl -n test exec -it flagger-loadtester-xx-xx sh
```
Gerar erros HTTP 500:
```
watch curl http://podinfo-canary:9898/status/500
```
Gerar latência:
```
watch curl http://podinfo-canary:9898/delay/1
```
Quando o número de verificações com falha atinge o limite de análise canary, o tráfego é roteado de volta para o principal, o canary é dimensionado para zero e a distribuição é marcada como falha.
```
kubectl -n test describe canary/podinfo

Status:
  Canary Weight:         0
  Failed Checks:         10
  Phase:                 Failed
Events:
  Type     Reason  Age   From     Message
  ----     ------  ----  ----     -------
  Normal   Synced  3m    flagger  Starting canary deployment for podinfo.test
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 5
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 10
  Normal   Synced  3m    flagger  Advance podinfo.test canary weight 15
  Normal   Synced  3m    flagger  Halt podinfo.test advancement success rate 69.17% < 99%
  Normal   Synced  2m    flagger  Halt podinfo.test advancement success rate 61.39% < 99%
  Normal   Synced  2m    flagger  Halt podinfo.test advancement success rate 55.06% < 99%
  Normal   Synced  2m    flagger  Halt podinfo.test advancement success rate 47.00% < 99%
  Normal   Synced  2m    flagger  (combined from similar events): Halt podinfo.test advancement success rate 38.08% < 99%
  Warning  Synced  1m    flagger  Rolling back podinfo.test failed checks threshold reached 10
  Warning  Synced  1m    flagger  Canary failed! Scaling down podinfo.test
```

Afinidade de sessão
Enquanto o Flagger pode executar roteamento ponderado e testes A/B individualmente, com o Istio ele pode combinar os dois levando a um Canário lançamento com afinidade de sessão. Para obter mais informações, você pode ler os documentos de estratégias de implantação.

Crie um recurso personalizado canário (substitua app.example.com por seu próprio domínio):
```
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: podinfo
  namespace: test
spec:
  # deployment reference
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  # the maximum time in seconds for the canary deployment
  # to make progress before it is rollback (default 600s)
  progressDeadlineSeconds: 60
  # HPA reference (optional)
  autoscalerRef:
    apiVersion: autoscaling/v2beta2
    kind: HorizontalPodAutoscaler
    name: podinfo
  service:
    # service port number
    port: 9898
    # container port number or name (optional)
    targetPort: 9898
    # Istio gateways (optional)
    gateways:
    - public-gateway.istio-system.svc.cluster.local
    # Istio virtual service host names (optional)
    hosts:
    - app.example.com
    # Istio traffic policy (optional)
    trafficPolicy:
      tls:
        # use ISTIO_MUTUAL when mTLS is enabled
        mode: DISABLE
    # Istio retry policy (optional)
    retries:
      attempts: 3
      perTryTimeout: 1s
      retryOn: "gateway-error,connect-failure,refused-stream"
  analysis:
    # schedule interval (default 60s)
    interval: 1m
    # max number of failed metric checks before rollback
    threshold: 5
    # max traffic percentage routed to canary
    # percentage (0-100)
    maxWeight: 50
    # canary increment step
    # percentage (0-100)
    stepWeight: 10
    # session affinity config
    sessionAffinity:
      # name of the cookie used
      cookieName: flagger-cookie
      # max age of the cookie (in seconds)
      # optional; defaults to 86400
      maxAge: 21600
    metrics:
    - name: request-success-rate
      # minimum req success rate (non 5xx responses)
      # percentage (0-100)
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      # maximum req duration P99
      # milliseconds
      thresholdRange:
        max: 500
      interval: 30s
    # testing (optional)
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 30s
        metadata:
          type: bash
          cmd: "curl -sd 'test' http://podinfo-canary:9898/token | grep token"
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://podinfo-canary.test:9898/"
```
Salve o recurso acima como podinfo-canary-session-affinity.yaml e aplique-o:
```
kubectl apply -f ./podinfo-canary-session-affinity.yaml
```
Acione uma implantação canária atualizando a imagem do contêiner:
```
kubectl -n test set image deployment/podinfo \
podinfod=ghcr.io/stefanprodan/podinfo:6.0.1
```
Você pode carregar em seu navegador e atualizá-lo até ver as solicitações sendo atendidas pelo . Todas as solicitações subsequentes serão atendidas por e não por afinidade de sessão configurado pelo Flagger com o Istio.app.example.compodinfo:6.0.1podinfo:6.0.1podinfo:6.0.0

Espelhamento de tráfego
Sinalizador Canário Traffic Shadowing

Para aplicativos que executam operações de leitura, o Flagger pode ser configurado para conduzir versões canárias com espelhamento de tráfego. O espelhamento de tráfego do Istio copiará cada solicitação recebida, enviando uma solicitação para o serviço principal e outra para o serviço canário. A resposta do primário é enviada de volta ao usuário e a resposta do canário é descartada. As métricas são coletadas em ambas as solicitações para que a implantação só prossiga se as métricas canárias estiverem dentro dos valores limite.

Observe que o espelhamento deve ser usado para solicitações idempotentes ou capazes de serem processadas duas vezes (uma pelo primário e outra pelo canário).

Você pode habilitar o espelhamento substituindo por e definindo como:stepWeight/maxWeightiterationsanalysis.mirrortrue
```
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: podinfo
  namespace: test
spec:
  analysis:
    # schedule interval
    interval: 1m
    # max number of failed metric checks before rollback
    threshold: 5
    # total number of iterations
    iterations: 10
    # enable traffic shadowing 
    mirror: true
    # weight of the traffic mirrored to your canary (defaults to 100%)
    mirrorWeight: 100
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 30s
        metadata:
          type: bash
          cmd: "curl -sd 'test' http://podinfo-canary:9898/token | grep token"
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://podinfo.test:9898/"

```
Com a configuração acima, o Flagger executará uma versão canary com as seguintes etapas:

- Detectar nova revisão (especificações de implantação, segredos ou alterações no configmaps)
- Dimensione a partir do zero a implantação do Canário
- aguarde até que o HPA defina as réplicas mínimas do canário
- verificar a saúde das vagens canárias
- executar os testes de aceitação
- abortar a liberação canária se os testes falharem
- iniciar os testes de carga
- Espelhar 100% do tráfego de Primary para Canary
- Verifique a taxa de sucesso da solicitação e a duração da solicitação a cada minuto
- Abortar a liberação do Canary se o limite de falha da verificação de métricas for atingido
- Interromper o espelhamento de tráfego após o número de iterações ser atingido
- Rotear o tráfego ao vivo para as cápsulas das Canárias
- Promover o Canary (atualizar os segredos primários, ConfigMaps e especificações de implantação)
- Aguarde a conclusão da implantação primária aguarde até que o HPA defina as réplicas mínimas primárias
- Verificar a integridade das vagens primárias
- Alternar o tráfego ao vivo de volta para o primário escala para zerar o canário
- Enviar notificação com o resultado da análise canária

O procedimento acima pode ser estendido com verificações de métricas personalizadas, webhooks, aprovação de promoção manual e notificações do Slack ou MS Teams.